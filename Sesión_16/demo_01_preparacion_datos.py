"""
Demo 01: Preparaci√≥n de Datos para Machine Learning
====================================================

Este demo muestra las t√©cnicas fundamentales para preparar datos petroleros
antes de entrenar modelos de machine learning.

Conceptos cubiertos:
- Exploraci√≥n inicial de datasets
- Limpieza y preprocesamiento
- Feature engineering espec√≠fico para datos petroleros
- Divisi√≥n de datos para entrenamiento/prueba
- Normalizaci√≥n y escalamiento
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

print("DEMO 01: PREPARACI√ìN DE DATOS PARA ML")
print("=" * 50)

# PASO 1: Cargar y explorar los datos
print("\nüîç PASO 1: EXPLORACI√ìN INICIAL")
print("-" * 30)

# Cargar dataset de producci√≥n
df = pd.read_csv('/workspaces/CursoPython-Basico-JuanDavid/Sesi√≥n_16/datos/produccion_historica.csv')

print(f"üìä Dataset cargado: {df.shape[0]} filas √ó {df.shape[1]} columnas")
print("\nüìã Primeras 5 filas:")
print(df.head())

print("\nüìà Informaci√≥n general:")
print(df.info())

print("\nüî¢ Estad√≠sticas descriptivas:")
print(df.describe())

# PASO 2: An√°lisis de calidad de datos
print("\nüîç PASO 2: AN√ÅLISIS DE CALIDAD")
print("-" * 30)

# Valores faltantes
print("‚ùå Valores faltantes por columna:")
missing = df.isnull().sum()
missing_pct = (missing / len(df)) * 100
for col, count in missing.items():
    if count > 0:
        print(f"   {col}: {count} ({missing_pct[col]:.1f}%)")

# Duplicados
duplicates = df.duplicated().sum()
print(f"\nüîÑ Filas duplicadas: {duplicates}")

# Valores an√≥malos b√°sicos
print("\n‚ö†Ô∏è  Detecci√≥n de valores an√≥malos:")
numeric_cols = df.select_dtypes(include=[np.number]).columns
for col in numeric_cols:
    if col != 'dias_operacion':  # Excluir contador de d√≠as
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
        if outliers > 0:
            print(f"   {col}: {outliers} outliers detectados")

# PASO 3: Limpieza de datos
print("\nüßπ PASO 3: LIMPIEZA DE DATOS")
print("-" * 30)

# Crear una copia para trabajar
df_clean = df.copy()

# Manejar valores faltantes
print("üîß Aplicando estrategias de imputaci√≥n:")

# Para presi√≥n: usar forward fill (valor anterior)
if df_clean['presion_boca_psi'].isnull().sum() > 0:
    before = df_clean['presion_boca_psi'].isnull().sum()
    df_clean['presion_boca_psi'].fillna(method='ffill', inplace=True)
    df_clean['presion_boca_psi'].fillna(df_clean['presion_boca_psi'].mean(), inplace=True)
    after = df_clean['presion_boca_psi'].isnull().sum()
    print(f"   ‚úì presion_boca_psi: {before} ‚Üí {after} valores faltantes")

# Para temperatura: usar interpolaci√≥n lineal
if df_clean['temperatura_f'].isnull().sum() > 0:
    before = df_clean['temperatura_f'].isnull().sum()
    df_clean['temperatura_f'].interpolate(method='linear', inplace=True)
    df_clean['temperatura_f'].fillna(df_clean['temperatura_f'].mean(), inplace=True)
    after = df_clean['temperatura_f'].isnull().sum()
    print(f"   ‚úì temperatura_f: {before} ‚Üí {after} valores faltantes")

# PASO 4: Feature Engineering
print("\n‚öôÔ∏è PASO 4: INGENIER√çA DE FEATURES")
print("-" * 30)

print("üî® Creando nuevas features:")

# 1. Features temporales
df_clean['fecha'] = pd.to_datetime(df_clean['fecha'])
df_clean['dia_semana'] = df_clean['fecha'].dt.dayofweek
df_clean['mes'] = df_clean['fecha'].dt.month
df_clean['trimestre'] = df_clean['fecha'].dt.quarter
print("   ‚úì Features temporales: dia_semana, mes, trimestre")

# 2. Ratios y relaciones
df_clean['ratio_gas_oil'] = df_clean['produccion_gas_mcf'] / df_clean['produccion_oil_bbl']
df_clean['corte_agua'] = df_clean['produccion_agua_bbl'] / (
    df_clean['produccion_oil_bbl'] + df_clean['produccion_agua_bbl']
) * 100
print("   ‚úì Ratios: ratio_gas_oil, corte_agua")

# 3. Features de ventana m√≥vil (√∫ltimos 7 d√≠as)
df_clean = df_clean.sort_values('fecha')
df_clean['produccion_promedio_7d'] = df_clean['produccion_oil_bbl'].rolling(window=7, min_periods=1).mean()
df_clean['produccion_std_7d'] = df_clean['produccion_oil_bbl'].rolling(window=7, min_periods=1).std()
df_clean['tendencia_7d'] = df_clean['produccion_oil_bbl'].rolling(window=7).apply(
    lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == 7 else 0
)
print("   ‚úì Features de ventana: promedio_7d, std_7d, tendencia_7d")

# 4. Categorizaci√≥n de choke size
def categorizar_choke(size):
    if size <= 28:
        return 'peque√±o'
    elif size <= 36:
        return 'mediano'
    else:
        return 'grande'

df_clean['choke_categoria'] = df_clean['choke_size'].apply(categorizar_choke)
print("   ‚úì Feature categ√≥rica: choke_categoria")

# 5. Indicadores de performance
df_clean['performance_index'] = (
    df_clean['produccion_oil_bbl'] / 
    (df_clean['presion_boca_psi'] / 1000 * df_clean['temperatura_f'] / 100)
)
print("   ‚úì √çndice de performance creado")

# PASO 5: Visualizaci√≥n de features
print("\nüìä PASO 5: VISUALIZACI√ìN DE FEATURES")
print("-" * 30)

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.ravel()

# Plot 1: Producci√≥n vs Presi√≥n
axes[0].scatter(df_clean['presion_boca_psi'], df_clean['produccion_oil_bbl'], alpha=0.6)
axes[0].set_xlabel('Presi√≥n Boca (psi)')
axes[0].set_ylabel('Producci√≥n Oil (bbl)')
axes[0].set_title('Relaci√≥n Presi√≥n vs Producci√≥n')

# Plot 2: Producci√≥n vs Temperatura
axes[1].scatter(df_clean['temperatura_f'], df_clean['produccion_oil_bbl'], alpha=0.6, color='red')
axes[1].set_xlabel('Temperatura (¬∞F)')
axes[1].set_ylabel('Producci√≥n Oil (bbl)')
axes[1].set_title('Relaci√≥n Temperatura vs Producci√≥n')

# Plot 3: Producci√≥n por categor√≠a de choke
df_clean.boxplot(column='produccion_oil_bbl', by='choke_categoria', ax=axes[2])
axes[2].set_xlabel('Categor√≠a de Choke')
axes[2].set_ylabel('Producci√≥n Oil (bbl)')
axes[2].set_title('Producci√≥n por Categor√≠a de Choke')

# Plot 4: Tendencia temporal
axes[3].plot(df_clean['fecha'].tail(100), df_clean['produccion_oil_bbl'].tail(100))
axes[3].set_xlabel('Fecha')
axes[3].set_ylabel('Producci√≥n Oil (bbl)')
axes[3].set_title('Tendencia Temporal (√∫ltimos 100 d√≠as)')
axes[3].tick_params(axis='x', rotation=45)

# Plot 5: Correlaci√≥n entre features num√©ricas
numeric_features = ['presion_boca_psi', 'temperatura_f', 'produccion_oil_bbl', 
                   'ratio_gas_oil', 'corte_agua', 'performance_index']
correlation = df_clean[numeric_features].corr()
im = axes[4].imshow(correlation, cmap='coolwarm', aspect='auto')
axes[4].set_xticks(range(len(numeric_features)))
axes[4].set_yticks(range(len(numeric_features)))
axes[4].set_xticklabels(numeric_features, rotation=45)
axes[4].set_yticklabels(numeric_features)
axes[4].set_title('Matriz de Correlaci√≥n')

# Plot 6: Distribuci√≥n de la variable objetivo
axes[5].hist(df_clean['produccion_oil_bbl'], bins=30, edgecolor='black', alpha=0.7)
axes[5].set_xlabel('Producci√≥n Oil (bbl)')
axes[5].set_ylabel('Frecuencia')
axes[5].set_title('Distribuci√≥n de Producci√≥n')

plt.tight_layout()
plt.savefig('exploracion_features.png', dpi=100, bbox_inches='tight')
print("üìà Visualizaciones guardadas como 'exploracion_features.png'")

# PASO 6: Preparar para ML
print("\nü§ñ PASO 6: PREPARACI√ìN FINAL PARA ML")
print("-" * 30)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Definir features y target
features_numericas = [
    'presion_boca_psi', 'temperatura_f', 'dias_operacion', 
    'choke_size', 'ratio_gas_oil', 'corte_agua',
    'produccion_promedio_7d', 'performance_index',
    'dia_semana', 'mes', 'trimestre'
]

features_categoricas = ['choke_categoria']
target = 'produccion_oil_bbl'

print(f"üìù Features seleccionadas:")
print(f"   ‚Ä¢ Num√©ricas: {len(features_numericas)}")
print(f"   ‚Ä¢ Categ√≥ricas: {len(features_categoricas)}")
print(f"   ‚Ä¢ Target: {target}")

# Preparar datos finales
df_final = df_clean.dropna(subset=features_numericas + [target])
print(f"\n‚úÖ Dataset final: {len(df_final)} registros (se eliminaron {len(df_clean) - len(df_final)} por NaN)")

# Codificar variables categ√≥ricas
le = LabelEncoder()
for col in features_categoricas:
    df_final[f'{col}_encoded'] = le.fit_transform(df_final[col])

# Preparar matrices finales
X_num = df_final[features_numericas]
X_cat = df_final[[f'{col}_encoded' for col in features_categoricas]]
X = pd.concat([X_num, X_cat], axis=1)
y = df_final[target]

print(f"‚úÖ Matriz de features X: {X.shape}")
print(f"‚úÖ Vector target y: {y.shape}")

# Divisi√≥n train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=None
)

print(f"üìä Divisi√≥n de datos:")
print(f"   ‚Ä¢ Entrenamiento: {len(X_train)} registros")
print(f"   ‚Ä¢ Prueba: {len(X_test)} registros")

# Escalamiento
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("‚öñÔ∏è Escalamiento aplicado con StandardScaler")

# Mostrar estad√≠sticas del escalamiento
print(f"\nEstad√≠sticas despu√©s del escalamiento:")
print(f"   ‚Ä¢ Media de features de entrenamiento: {X_train_scaled.mean(axis=0).mean():.3f}")
print(f"   ‚Ä¢ Std de features de entrenamiento: {X_train_scaled.std(axis=0).mean():.3f}")

# RESUMEN FINAL
print("\n" + "="*50)
print("üìã RESUMEN DEL PREPROCESSING")
print("="*50)

print(f"üî¢ Dataset original: {df.shape[0]} registros")
print(f"üßπ Dataset limpio: {len(df_clean)} registros")
print(f"‚úÖ Dataset final: {len(df_final)} registros")
print(f"üìä Features creadas: {len(X.columns)} total")
print(f"üéØ Listo para entrenamiento de modelos")

print("\nüí° Pr√≥ximos pasos:")
print("   1. Entrenar modelo de regresi√≥n")
print("   2. Validar con datos de prueba")
print("   3. Evaluar m√©tricas de performance")

print(f"\n‚úÖ Demo completado. Contin√∫a con demo_02_regresion_produccion.py")